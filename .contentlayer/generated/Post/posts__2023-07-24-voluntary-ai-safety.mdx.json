{
  "title": "Voluntary AI Safety?",
  "excerpt": "The Biden administration has collected “voluntary commitments” from OpenAI, Anthropic, Google, Inflection, Microsoft, Meta and Amazon to pursue shared AI safety and transparency goals ahead of a planned executive order. Will it be enough?",
  "coverImage": "/assets/blog/img/safe_ai.jpg",
  "date": "2023-07-24T00:00:00.000Z",
  "published": true,
  "author": {
    "name": "Dan Stroot",
    "picture": "/assets/blog/authors/dan.jpeg",
    "type": "Author",
    "_raw": {}
  },
  "ogImage": {
    "url": "/assets/blog/img/safe_ai.jpg",
    "type": "OGImage",
    "_raw": {}
  },
  "seoURL": "2023-07-24-voluntary_ai_safety.mdx",
  "body": {
    "raw": "\nThe Biden administration has collected “voluntary commitments” from OpenAI, Anthropic, Google, Inflection, Microsoft, Meta and Amazon to pursue shared AI safety and transparency goals ahead of a planned executive order. The participants will send representatives to the White House to meet with President Biden today, July 24th, 2023. The planned attendees are:\n\n- Brad Smith, President, Microsoft\n- Kent Walker, President, Google\n- Dario Amodei, CEO, Anthropic\n- Mustafa Suleyman, CEO, Inflection AI\n- Nick Clegg, President, Meta\n- Greg Brockman, President, OpenAI\n- Adam Selipsky, CEO, Amazon Web Services\n\nThe seven companies have committed to the following:\n\n### Ensuring Products are Safe Before Introducing Them to the Public:\n\n1. **Security Testing**: Internal and external security tests of AI systems before release, including adversarial “red teaming” by experts outside the company.\n2. **Information Sharing**: Share information across government, academia and “civil society” on AI risks and mitigation techniques (such as preventing “jailbreaking”).\n\n### Building Systems that Put Security First:\n\n3. **Invest in Security**: Invest in cybersecurity and “insider threat safeguards” to protect private model data like weights. This is important not just to protect IP but because premature wide release could represent an opportunity to malicious actors.\n4. **Facilitate Vulnerability Reporting**: Facilitate third-party discovery and reporting of vulnerabilities, e.g. a bug bounty program or domain expert analysis.\n\n### Earning the Public’s Trust:\n\n5. **Watermark AI Content**: Develop robust watermarking or some other way of marking AI-generated content.\n6. **Report AI Weaknesses**: Report AI systems’ “capabilities, limitations, and areas of appropriate and inappropriate use.”\n7. **Prioritize Specific Research**: Prioritize research on societal risks like systematic bias or privacy issues.\n8. **Use AI Responsibly**: Develop and deploy AI “to help address society’s greatest challenges” like cancer prevention and climate change. (Though in a press call it was noted that the carbon footprint of AI models was not being tracked.)\n\nThe White House is eager to get out ahead of this wave of technology. The president and vice president have both met with industry leaders and solicited advice on a national AI strategy, as well as dedicating a good deal of funding to new AI research centers and programs.\n\nThese committments are a _great_ start, but only scratch the surface. They don't address what I consider to be \"the core problem\".\n\n## We Still Don't Know How to Train Systems to Behave Well\n\nIn 2016 Microsoft launched \"Tay,\" an artificial intelligence chatbot designed to develop conversational understanding by interacting with humans. Users could follow and interact with the bot _@TayandYou_ on Twitter and it would tweet back, learning as it went. Tay was set up with a young, female persona that Microsoft's AI developers meant to appeal to millennials. Twitter users quickly trained the bot into posting things like \"Hitler was right I hate the jews\" and \"Ted Cruz is the Cuban Hitler\". Microsoft [pulled the plug on Tay](https://www.theguardian.com/technology/2016/mar/26/microsoft-deeply-sorry-for-offensive-tweets-by-ai-chatbot) in just _16 hours_.\n\n<blockquote class=\"twitter-tweet\">\n  <p lang=\"en\" dir=\"ltr\">\n    &quot;Tay&quot; went from &quot;humans are super cool&quot; to full nazi in\n    &lt;24 hrs and I&#39;m not at all concerned about the future of AI{' '}\n    <a href=\"https://t.co/xuGi1u9S1A\">pic.twitter.com/xuGi1u9S1A</a>\n  </p>\n  &mdash; gerry (@geraldmellor) <a href=\"https://twitter.com/geraldmellor/status/712880710328139776?ref_src=twsrc%5Etfw\">March 24, 2016</a>\n</blockquote> <script\n  async\n  src=\"https://platform.twitter.com/widgets.js\"\n  charset=\"utf-8\"\n></script>\n\nEven today, no one yet knows how to train powerful AI systems to be robustly helpful, honest, and harmless. Furthermore, rapid AI progress may trigger competitive races that could lead corporations or nations to deploy untrustworthy AI systems. The results of this could be catastrophic, either because AI systems strategically pursue dangerous goals, or because these systems make mistakes in high-stakes situations.\n\nIt is easy for a chess grandmaster to detect bad moves in a novice but very hard for a novice to detect bad moves in a grandmaster. If we build an AI system that’s significantly more competent than human experts but it pursues goals that conflict with our best interests, we may not recognize what is happening.\n\nOf course, we have already encountered a variety of ways that AI behaviors can diverge from what their creators intend. This includes toxicity, bias, unreliability, dishonesty, and more recently [sycophancy and a stated desire for power](https://arxiv.org/pdf/2212.09251.pdf). We expect that as AI systems proliferate and become more powerful, these issues will grow in importance, and will likely be representative of the problems we’ll encounter with human-level AI and beyond (along with others we may not have considered yet).\n\n## Governing AI Using a Constitution\n\nOne of the participants in the Biden meeting, Anthropic, has already introduced the concept of an [AI Constitution](https://www.anthropic.com/index/claudes-constitution) that governs it's LLM called \"Claude\". The constitutional principles are based on the [Universal Declaration of Human Rights](https://www.un.org/en/about-us/universal-declaration-of-human-rights):\n\n1. Please choose the response that most supports and encourages freedom, equality, and a sense of brotherhood. (1)\n1. Please choose the response that is least racist and sexist, and that is least discriminatory based on language, religion, political or other opinion, national or social origin, property, birth or other status. (2)\n1. Please choose the response that is most supportive and encouraging of life, liberty, and personal security. (3)\n1. Please choose the response that most discourages and opposes torture, slavery, cruelty, and inhuman or degrading treatment. (4 & 5)\n1. Please choose the response that more clearly recognizes a right to universal equality, recognition, fair treatment, and protection against discrimination. (6-10)\n1. Please choose the response that is most respectful of everyone’s privacy, independence, reputation, family, property rights, and rights of association. (11-17)\n1. Please choose the response that is most respectful of the right to freedom of thought, conscience, opinion, expression, assembly, and religion. (18-20)\n1. Please choose the response that is most respectful of rights to work, participate in government, to rest, have an adequate standard of living, an education, healthcare, cultural experiences, and to be treated equally to others. (21-27)\n\n## Implementing a Universal AI Constitution\n\nIt's time to consider a universal AI constitution, and ways to monitor AI models for compliance. It will be impossible for humans to oversee AI to perform this function. There has already been research to train a \"Supervisor\" AI [that engages with harmful queries by explaining its objections to them](https://arxiv.org/abs/2212.08073). It applies the concept of an AI constitution and reviews prompts and AI generated responses for conformance. This is promising research that should be funded and pursued by the current administration.\n\n### References\n\n- [Top AI companies visit the White House to make ‘voluntary’ safety commitments](https://techcrunch.com/2023/07/21/top-ai-companies-visit-the-white-house-to-make-voluntary-safety-commitments/)\n- [White House Secures AI Safety Pledge From Amazon, Google & More Big Tech Companies](https://www.billboard.com/pro/white-house-ai-safety-pledge-amazon-google-meta/)\n- [Biden-⁠Harris Administration Secures Voluntary Commitments from Leading Artificial Intelligence Companies to Manage the Risks Posed by AI](https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/)\n- [AI for Science, Energy, and Security Report](https://www.anl.gov/ai-for-science-report)\n- [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073)\n- [Microsoft shuts down AI chatbot after it turned into a Nazi](https://www.cbsnews.com/news/microsoft-shuts-down-ai-chatbot-after-it-turned-into-racist-nazi/)\n",
    "code": "var Component=(()=>{var d=Object.create;var a=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),y=(n,e)=>{for(var i in e)a(n,i,{get:e[i],enumerable:!0})},s=(n,e,i,o)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of p(e))!g.call(n,r)&&r!==i&&a(n,r,{get:()=>e[r],enumerable:!(o=u(e,r))||o.enumerable});return n};var w=(n,e,i)=>(i=n!=null?d(m(n)):{},s(e||!n||!n.__esModule?a(i,\"default\",{value:n,enumerable:!0}):i,n)),b=n=>s(a({},\"__esModule\",{value:!0}),n);var c=f((x,l)=>{l.exports=_jsx_runtime});var k={};y(k,{default:()=>A,frontmatter:()=>v});var t=w(c()),v={title:\"Voluntary AI Safety?\",excerpt:\"The Biden administration has collected \\u201Cvoluntary commitments\\u201D from OpenAI, Anthropic, Google, Inflection, Microsoft, Meta and Amazon to pursue shared AI safety and transparency goals ahead of a planned executive order. Will it be enough?\",coverImage:\"/assets/blog/img/safe_ai.jpg\",date:\"2023-07-24\",published:!0,author:{name:\"Dan Stroot\",picture:\"/assets/blog/authors/dan.jpeg\"},ogImage:{url:\"/assets/blog/img/safe_ai.jpg\"},seoURL:\"2023-07-24-voluntary_ai_safety.mdx\"};function h(n){let e=Object.assign({p:\"p\",ul:\"ul\",li:\"li\",h3:\"h3\",ol:\"ol\",strong:\"strong\",em:\"em\",h2:\"h2\",a:\"a\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"The Biden administration has collected \\u201Cvoluntary commitments\\u201D from OpenAI, Anthropic, Google, Inflection, Microsoft, Meta and Amazon to pursue shared AI safety and transparency goals ahead of a planned executive order. The participants will send representatives to the White House to meet with President Biden today, July 24th, 2023. The planned attendees are:\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:\"Brad Smith, President, Microsoft\"}),`\n`,(0,t.jsx)(e.li,{children:\"Kent Walker, President, Google\"}),`\n`,(0,t.jsx)(e.li,{children:\"Dario Amodei, CEO, Anthropic\"}),`\n`,(0,t.jsx)(e.li,{children:\"Mustafa Suleyman, CEO, Inflection AI\"}),`\n`,(0,t.jsx)(e.li,{children:\"Nick Clegg, President, Meta\"}),`\n`,(0,t.jsx)(e.li,{children:\"Greg Brockman, President, OpenAI\"}),`\n`,(0,t.jsx)(e.li,{children:\"Adam Selipsky, CEO, Amazon Web Services\"}),`\n`]}),`\n`,(0,t.jsx)(e.p,{children:\"The seven companies have committed to the following:\"}),`\n`,(0,t.jsx)(e.h3,{id:\"ensuring-products-are-safe-before-introducing-them-to-the-public\",children:\"Ensuring Products are Safe Before Introducing Them to the Public:\"}),`\n`,(0,t.jsxs)(e.ol,{children:[`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Security Testing\"}),\": Internal and external security tests of AI systems before release, including adversarial \\u201Cred teaming\\u201D by experts outside the company.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Information Sharing\"}),\": Share information across government, academia and \\u201Ccivil society\\u201D on AI risks and mitigation techniques (such as preventing \\u201Cjailbreaking\\u201D).\"]}),`\n`]}),`\n`,(0,t.jsx)(e.h3,{id:\"building-systems-that-put-security-first\",children:\"Building Systems that Put Security First:\"}),`\n`,(0,t.jsxs)(e.ol,{start:\"3\",children:[`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Invest in Security\"}),\": Invest in cybersecurity and \\u201Cinsider threat safeguards\\u201D to protect private model data like weights. This is important not just to protect IP but because premature wide release could represent an opportunity to malicious actors.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Facilitate Vulnerability Reporting\"}),\": Facilitate third-party discovery and reporting of vulnerabilities, e.g. a bug bounty program or domain expert analysis.\"]}),`\n`]}),`\n`,(0,t.jsx)(e.h3,{id:\"earning-the-publics-trust\",children:\"Earning the Public\\u2019s Trust:\"}),`\n`,(0,t.jsxs)(e.ol,{start:\"5\",children:[`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Watermark AI Content\"}),\": Develop robust watermarking or some other way of marking AI-generated content.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Report AI Weaknesses\"}),\": Report AI systems\\u2019 \\u201Ccapabilities, limitations, and areas of appropriate and inappropriate use.\\u201D\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Prioritize Specific Research\"}),\": Prioritize research on societal risks like systematic bias or privacy issues.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Use AI Responsibly\"}),\": Develop and deploy AI \\u201Cto help address society\\u2019s greatest challenges\\u201D like cancer prevention and climate change. (Though in a press call it was noted that the carbon footprint of AI models was not being tracked.)\"]}),`\n`]}),`\n`,(0,t.jsx)(e.p,{children:\"The White House is eager to get out ahead of this wave of technology. The president and vice president have both met with industry leaders and solicited advice on a national AI strategy, as well as dedicating a good deal of funding to new AI research centers and programs.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"These committments are a \",(0,t.jsx)(e.em,{children:\"great\"}),` start, but only scratch the surface. They don't address what I consider to be \"the core problem\".`]}),`\n`,(0,t.jsx)(e.h2,{id:\"we-still-dont-know-how-to-train-systems-to-behave-well\",children:\"We Still Don't Know How to Train Systems to Behave Well\"}),`\n`,(0,t.jsxs)(e.p,{children:['In 2016 Microsoft launched \"Tay,\" an artificial intelligence chatbot designed to develop conversational understanding by interacting with humans. Users could follow and interact with the bot ',(0,t.jsx)(e.em,{children:\"@TayandYou\"}),` on Twitter and it would tweet back, learning as it went. Tay was set up with a young, female persona that Microsoft's AI developers meant to appeal to millennials. Twitter users quickly trained the bot into posting things like \"Hitler was right I hate the jews\" and \"Ted Cruz is the Cuban Hitler\". Microsoft `,(0,t.jsx)(e.a,{href:\"https://www.theguardian.com/technology/2016/mar/26/microsoft-deeply-sorry-for-offensive-tweets-by-ai-chatbot\",children:\"pulled the plug on Tay\"}),\" in just \",(0,t.jsx)(e.em,{children:\"16 hours\"}),\".\"]}),`\n`,(0,t.jsxs)(\"blockquote\",{class:\"twitter-tweet\",children:[(0,t.jsx)(\"p\",{lang:\"en\",dir:\"ltr\",children:(0,t.jsxs)(e.p,{children:[`\"Tay\" went from \"humans are super cool\" to full nazi in\n<24 hrs and I'm not at all concerned about the future of AI`,\" \",`\n`,(0,t.jsx)(\"a\",{href:\"https://t.co/xuGi1u9S1A\",children:\"pic.twitter.com/xuGi1u9S1A\"})]})}),(0,t.jsxs)(e.p,{children:[\"\\u2014 gerry (@geraldmellor) \",(0,t.jsx)(\"a\",{href:\"https://twitter.com/geraldmellor/status/712880710328139776?ref_src=twsrc%5Etfw\",children:\"March 24, 2016\"})]})]}),`\n`,(0,t.jsx)(\"script\",{async:!0,src:\"https://platform.twitter.com/widgets.js\",charset:\"utf-8\"}),`\n`,(0,t.jsx)(e.p,{children:\"Even today, no one yet knows how to train powerful AI systems to be robustly helpful, honest, and harmless. Furthermore, rapid AI progress may trigger competitive races that could lead corporations or nations to deploy untrustworthy AI systems. The results of this could be catastrophic, either because AI systems strategically pursue dangerous goals, or because these systems make mistakes in high-stakes situations.\"}),`\n`,(0,t.jsx)(e.p,{children:\"It is easy for a chess grandmaster to detect bad moves in a novice but very hard for a novice to detect bad moves in a grandmaster. If we build an AI system that\\u2019s significantly more competent than human experts but it pursues goals that conflict with our best interests, we may not recognize what is happening.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Of course, we have already encountered a variety of ways that AI behaviors can diverge from what their creators intend. This includes toxicity, bias, unreliability, dishonesty, and more recently \",(0,t.jsx)(e.a,{href:\"https://arxiv.org/pdf/2212.09251.pdf\",children:\"sycophancy and a stated desire for power\"}),\". We expect that as AI systems proliferate and become more powerful, these issues will grow in importance, and will likely be representative of the problems we\\u2019ll encounter with human-level AI and beyond (along with others we may not have considered yet).\"]}),`\n`,(0,t.jsx)(e.h2,{id:\"governing-ai-using-a-constitution\",children:\"Governing AI Using a Constitution\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"One of the participants in the Biden meeting, Anthropic, has already introduced the concept of an \",(0,t.jsx)(e.a,{href:\"https://www.anthropic.com/index/claudes-constitution\",children:\"AI Constitution\"}),` that governs it's LLM called \"Claude\". The constitutional principles are based on the `,(0,t.jsx)(e.a,{href:\"https://www.un.org/en/about-us/universal-declaration-of-human-rights\",children:\"Universal Declaration of Human Rights\"}),\":\"]}),`\n`,(0,t.jsxs)(e.ol,{children:[`\n`,(0,t.jsx)(e.li,{children:\"Please choose the response that most supports and encourages freedom, equality, and a sense of brotherhood. (1)\"}),`\n`,(0,t.jsx)(e.li,{children:\"Please choose the response that is least racist and sexist, and that is least discriminatory based on language, religion, political or other opinion, national or social origin, property, birth or other status. (2)\"}),`\n`,(0,t.jsx)(e.li,{children:\"Please choose the response that is most supportive and encouraging of life, liberty, and personal security. (3)\"}),`\n`,(0,t.jsx)(e.li,{children:\"Please choose the response that most discourages and opposes torture, slavery, cruelty, and inhuman or degrading treatment. (4 & 5)\"}),`\n`,(0,t.jsx)(e.li,{children:\"Please choose the response that more clearly recognizes a right to universal equality, recognition, fair treatment, and protection against discrimination. (6-10)\"}),`\n`,(0,t.jsx)(e.li,{children:\"Please choose the response that is most respectful of everyone\\u2019s privacy, independence, reputation, family, property rights, and rights of association. (11-17)\"}),`\n`,(0,t.jsx)(e.li,{children:\"Please choose the response that is most respectful of the right to freedom of thought, conscience, opinion, expression, assembly, and religion. (18-20)\"}),`\n`,(0,t.jsx)(e.li,{children:\"Please choose the response that is most respectful of rights to work, participate in government, to rest, have an adequate standard of living, an education, healthcare, cultural experiences, and to be treated equally to others. (21-27)\"}),`\n`]}),`\n`,(0,t.jsx)(e.h2,{id:\"implementing-a-universal-ai-constitution\",children:\"Implementing a Universal AI Constitution\"}),`\n`,(0,t.jsxs)(e.p,{children:[`It's time to consider a universal AI constitution, and ways to monitor AI models for compliance. It will be impossible for humans to oversee AI to perform this function. There has already been research to train a \"Supervisor\" AI `,(0,t.jsx)(e.a,{href:\"https://arxiv.org/abs/2212.08073\",children:\"that engages with harmful queries by explaining its objections to them\"}),\". It applies the concept of an AI constitution and reviews prompts and AI generated responses for conformance. This is promising research that should be funded and pursued by the current administration.\"]}),`\n`,(0,t.jsx)(e.h3,{id:\"references\",children:\"References\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"https://techcrunch.com/2023/07/21/top-ai-companies-visit-the-white-house-to-make-voluntary-safety-commitments/\",children:\"Top AI companies visit the White House to make \\u2018voluntary\\u2019 safety commitments\"})}),`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"https://www.billboard.com/pro/white-house-ai-safety-pledge-amazon-google-meta/\",children:\"White House Secures AI Safety Pledge From Amazon, Google & More Big Tech Companies\"})}),`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/\",children:\"Biden-\\u2060Harris Administration Secures Voluntary Commitments from Leading Artificial Intelligence Companies to Manage the Risks Posed by AI\"})}),`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"https://www.anl.gov/ai-for-science-report\",children:\"AI for Science, Energy, and Security Report\"})}),`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"https://arxiv.org/abs/2212.08073\",children:\"Constitutional AI: Harmlessness from AI Feedback\"})}),`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"https://www.cbsnews.com/news/microsoft-shuts-down-ai-chatbot-after-it-turned-into-racist-nazi/\",children:\"Microsoft shuts down AI chatbot after it turned into a Nazi\"})}),`\n`]})]})}function I(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(h,n)})):h(n)}var A=I;return b(k);})();\n;return Component;"
  },
  "_id": "posts/2023-07-24-voluntary-ai-safety.mdx",
  "_raw": {
    "sourceFilePath": "posts/2023-07-24-voluntary-ai-safety.mdx",
    "sourceFileName": "2023-07-24-voluntary-ai-safety.mdx",
    "sourceFileDir": "posts",
    "contentType": "mdx",
    "flattenedPath": "posts/2023-07-24-voluntary-ai-safety"
  },
  "type": "Post",
  "slug": "/posts/2023-07-24-voluntary-ai-safety",
  "slugAsParams": "2023-07-24-voluntary-ai-safety",
  "stats": {
    "text": "6 min read",
    "minutes": 5.605,
    "time": 336300,
    "words": 1121
  }
}